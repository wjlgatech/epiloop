{
  "project": "research-loop",
  "branchName": "feature/research-loop",
  "description": "Agentic research team that coordinates specialized AI agents to investigate complex questions, synthesize findings, and produce verified research reports. Built on claude-loop's orchestration layer.",
  "version": "0.1.0",
  "userStories": [
    {
      "id": "US-001",
      "title": "Core Research Orchestrator",
      "description": "As a user, I want to submit a research question and have it automatically decomposed into sub-questions that are delegated to specialist agents.",
      "acceptanceCriteria": [
        "Create research-loop.sh entry point that accepts a research question as input",
        "Implement question decomposition that breaks complex questions into 3-7 sub-questions",
        "Create research-state.json schema to track research progress (similar to prd.json)",
        "Implement agent delegation based on sub-question type (academic, market, technical, etc.)",
        "Create lib/research-orchestrator.py with main orchestration logic",
        "Tests pass for question decomposition with sample questions"
      ],
      "dependencies": [],
      "fileScope": ["research-loop.sh", "lib/research-orchestrator.py", "lib/question-decomposer.py", "schemas/research-state.json"],
      "estimatedComplexity": "medium",
      "suggestedModel": "sonnet",
      "priority": 1,
      "passes": false
    },
    {
      "id": "US-002",
      "title": "Web Search Integration",
      "description": "As a research agent, I want to search the web for relevant information so I can gather data from multiple sources.",
      "acceptanceCriteria": [
        "Create lib/search-provider.py with abstraction for multiple search backends",
        "Implement Tavily search integration (primary)",
        "Implement fallback to DuckDuckGo or SerpAPI if Tavily unavailable",
        "Return structured results with title, URL, snippet, and relevance score",
        "Cache search results to avoid redundant API calls",
        "Rate limiting to respect API quotas",
        "Tests pass with mock search responses"
      ],
      "dependencies": [],
      "fileScope": ["lib/search-provider.py", "lib/search-cache.py"],
      "estimatedComplexity": "simple",
      "suggestedModel": "haiku",
      "priority": 1,
      "passes": false
    },
    {
      "id": "US-003",
      "title": "Lead Researcher Agent",
      "description": "As a user, I want a Lead Researcher agent that orchestrates the research process, delegates to specialists, and synthesizes findings.",
      "acceptanceCriteria": [
        "Create agents/lead-researcher.md with agent specification and prompts",
        "Implement synthesis logic that combines findings from multiple agents",
        "Implement gap identification that detects missing information",
        "Implement confidence scoring for synthesized findings (0-100)",
        "Create lib/research-synthesizer.py for combining agent outputs",
        "Tests pass for synthesis with sample multi-agent findings"
      ],
      "dependencies": ["US-001"],
      "fileScope": ["agents/lead-researcher.md", "lib/research-synthesizer.py", "lib/confidence-scorer.py"],
      "estimatedComplexity": "medium",
      "suggestedModel": "sonnet",
      "priority": 2,
      "passes": false
    },
    {
      "id": "US-004",
      "title": "Domain Expert Agents (Academic, Technical, Market)",
      "description": "As a Lead Researcher, I want specialist agents for academic papers, technical docs, and market research so I can delegate appropriately.",
      "acceptanceCriteria": [
        "Create agents/academic-scanner.md - searches arXiv, Semantic Scholar, Google Scholar",
        "Create agents/technical-diver.md - searches GitHub, Stack Overflow, documentation sites",
        "Create agents/market-analyst.md - searches company info, news, pricing, competitive landscape",
        "Each agent returns structured findings with sources and confidence",
        "Implement agent selection based on sub-question keywords",
        "Tests pass for each agent type with sample queries"
      ],
      "dependencies": ["US-002"],
      "fileScope": ["agents/academic-scanner.md", "agents/technical-diver.md", "agents/market-analyst.md", "lib/agent-selector.py"],
      "estimatedComplexity": "medium",
      "suggestedModel": "sonnet",
      "priority": 2,
      "passes": false
    },
    {
      "id": "US-005",
      "title": "Source Credibility Evaluator",
      "description": "As a quality control measure, I want to score the credibility of sources so users know which findings to trust.",
      "acceptanceCriteria": [
        "Create lib/source-evaluator.py with credibility scoring logic",
        "Score based on: domain authority, author credentials, publication date, citation count",
        "Maintain source-credibility-store.json that tracks source reliability over time",
        "Flag low-credibility sources (score <50) in output",
        "Implement learning from human corrections (when user says source was wrong)",
        "Tests pass for known high/low credibility sources"
      ],
      "dependencies": ["US-002"],
      "fileScope": ["lib/source-evaluator.py", "data/source-credibility-store.json", "lib/credibility-learner.py"],
      "estimatedComplexity": "medium",
      "suggestedModel": "sonnet",
      "priority": 3,
      "passes": false
    },
    {
      "id": "US-006",
      "title": "Fact Checker Agent",
      "description": "As a quality control measure, I want a Fact Checker agent that verifies claims against multiple sources.",
      "acceptanceCriteria": [
        "Create agents/fact-checker.md with verification prompts",
        "Implement claim extraction from research findings",
        "Implement multi-source verification (require 2+ sources per claim)",
        "Flag unverified claims with confidence impact",
        "Create lib/claim-verifier.py with verification logic",
        "Tests pass for sample claims (both verified and unverified)"
      ],
      "dependencies": ["US-002", "US-005"],
      "fileScope": ["agents/fact-checker.md", "lib/claim-verifier.py", "lib/claim-extractor.py"],
      "estimatedComplexity": "medium",
      "suggestedModel": "sonnet",
      "priority": 3,
      "passes": false
    },
    {
      "id": "US-007",
      "title": "Devil's Advocate Agent",
      "description": "As a quality control measure, I want a Devil's Advocate agent that challenges conclusions and finds counterarguments.",
      "acceptanceCriteria": [
        "Create agents/devils-advocate.md with counterargument prompts",
        "Implement conclusion extraction from research synthesis",
        "Search for evidence contradicting main conclusions",
        "Propose alternative interpretations",
        "Rate strength of counterarguments (weak/moderate/strong)",
        "Tests pass for sample conclusions with known counterarguments"
      ],
      "dependencies": ["US-003"],
      "fileScope": ["agents/devils-advocate.md", "lib/counterargument-finder.py"],
      "estimatedComplexity": "simple",
      "suggestedModel": "haiku",
      "priority": 3,
      "passes": false
    },
    {
      "id": "US-008",
      "title": "AI-ML Research Adapter",
      "description": "As a user researching AI/ML topics, I want specialized agents that understand ML papers, benchmarks, and the AI ecosystem.",
      "acceptanceCriteria": [
        "Create adapters/ai-ml/adapter.yaml with domain configuration",
        "Implement arXiv API integration for ML papers (cs.AI, cs.LG, cs.CL, cs.CV)",
        "Implement Papers With Code integration for SOTA benchmarks",
        "Create agents/benchmark-analyst.md for tracking benchmark results",
        "Add AI-ML specific quality gates (reproducibility, benchmark validity)",
        "Tests pass with sample AI-ML research query from spec"
      ],
      "dependencies": ["US-004", "US-006"],
      "fileScope": ["adapters/ai-ml/adapter.yaml", "adapters/ai-ml/prompts/", "lib/arxiv-client.py", "lib/paperswithcode-client.py", "agents/benchmark-analyst.md"],
      "estimatedComplexity": "medium",
      "suggestedModel": "sonnet",
      "priority": 4,
      "passes": false
    },
    {
      "id": "US-009",
      "title": "Investment Research Adapter",
      "description": "As a user researching investment opportunities, I want specialized agents for stocks, crypto, and market analysis.",
      "acceptanceCriteria": [
        "Create adapters/investment/adapter.yaml with domain configuration",
        "Implement Yahoo Finance integration for stock data",
        "Implement CoinGecko integration for crypto data",
        "Create agents/fundamental-analyst.md for financial analysis",
        "Create agents/technical-analyst.md for chart patterns and momentum",
        "Create agents/risk-assessor.md for risk analysis",
        "Add investment-specific quality gates (recency, confirmation bias check)",
        "Add mandatory disclaimer injection for all investment outputs",
        "Tests pass with sample investment query from spec"
      ],
      "dependencies": ["US-004", "US-006", "US-007"],
      "fileScope": ["adapters/investment/adapter.yaml", "adapters/investment/prompts/", "lib/yahoo-finance-client.py", "lib/coingecko-client.py", "agents/fundamental-analyst.md", "agents/technical-analyst.md", "agents/risk-assessor.md"],
      "estimatedComplexity": "complex",
      "suggestedModel": "opus",
      "priority": 4,
      "passes": false
    },
    {
      "id": "US-010",
      "title": "Research Report Generator",
      "description": "As a user, I want research findings presented in a well-structured report with citations, confidence scores, and actionable insights.",
      "acceptanceCriteria": [
        "Create lib/report-generator.py with multiple output formats",
        "Generate Markdown reports with sections matching research flow",
        "Include inline citations linking to sources",
        "Include confidence scores for each major finding",
        "Include risk/disclaimer sections for investment research",
        "Generate executive summary for quick consumption",
        "Save reports to research-outputs/ directory",
        "Tests pass generating reports from sample research findings"
      ],
      "dependencies": ["US-003", "US-005", "US-006"],
      "fileScope": ["lib/report-generator.py", "templates/report-template.md", "templates/investment-report-template.md"],
      "estimatedComplexity": "medium",
      "suggestedModel": "sonnet",
      "priority": 4,
      "passes": false
    },
    {
      "id": "US-011",
      "title": "Human Checkpoint System",
      "description": "As a user, I want mandatory checkpoints for high-stakes research (investment, health) where I can review and approve before final output.",
      "acceptanceCriteria": [
        "Create lib/human-checkpoint.py with checkpoint logic",
        "Implement mandatory checkpoints for investment research",
        "Display key findings and confidence before proceeding",
        "Allow user to: approve, request more depth, redirect research, or cancel",
        "Log all checkpoint decisions for audit",
        "Integrate with research-loop.sh for interactive checkpoints",
        "Tests pass for checkpoint flow (approve, reject scenarios)"
      ],
      "dependencies": ["US-003"],
      "fileScope": ["lib/human-checkpoint.py", "lib/checkpoint-logger.py"],
      "estimatedComplexity": "simple",
      "suggestedModel": "haiku",
      "priority": 5,
      "passes": false
    },
    {
      "id": "US-012",
      "title": "Prediction Tracking & Learning",
      "description": "As a user, I want the system to track investment predictions and learn from outcomes to improve future research.",
      "acceptanceCriteria": [
        "Create lib/prediction-tracker.py to record all investment recommendations",
        "Store predictions with: asset, entry price, targets, stop-loss, timestamp",
        "Create lib/outcome-tracker.py to record actual outcomes",
        "Calculate hit rate, average return, Sharpe ratio",
        "Identify which signals/sources had predictive value",
        "Feed learnings back to source credibility and agent prompts",
        "Create CLI command: research-loop.sh --track-outcomes",
        "Tests pass for prediction recording and outcome tracking"
      ],
      "dependencies": ["US-009", "US-005"],
      "fileScope": ["lib/prediction-tracker.py", "lib/outcome-tracker.py", "data/predictions.json", "data/outcomes.json", "lib/learning-feedback.py"],
      "estimatedComplexity": "medium",
      "suggestedModel": "sonnet",
      "priority": 5,
      "passes": false
    },
    {
      "id": "US-013",
      "title": "Integration Tests & Documentation",
      "description": "As a developer, I want comprehensive tests and documentation so the system is maintainable and extensible.",
      "acceptanceCriteria": [
        "Create tests/test_ai_ml_research.py with AI-ML use case from spec",
        "Create tests/test_investment_research.py with investment use case from spec",
        "Create tests/test_quality_gates.py for all quality gates",
        "Create README.md for research-loop with usage examples",
        "Create docs/research-loop/USAGE.md with detailed guide",
        "All tests pass",
        "Documentation covers all CLI commands and options"
      ],
      "dependencies": ["US-008", "US-009", "US-010", "US-011", "US-012"],
      "fileScope": ["tests/", "README-research-loop.md", "docs/research-loop/USAGE.md"],
      "estimatedComplexity": "medium",
      "suggestedModel": "sonnet",
      "priority": 6,
      "passes": false
    }
  ],
  "metadata": {
    "createdAt": "2026-01-17T00:00:00Z",
    "totalStories": 13,
    "estimatedDuration": "6-8 weeks",
    "priority": "high",
    "tags": ["research", "multi-agent", "investment", "ai-ml", "flywheel"]
  }
}
