{
  "project": "self-improvement-infrastructure",
  "branchName": "feature/self-improvement",
  "description": "Add capability gap detection, pattern analysis, and autonomous PRD generation to claude-loop for continuous self-improvement",
  "parallelization": {
    "enabled": true,
    "maxWorkers": 2
  },
  "userStories": [
    {
      "id": "SI-001",
      "title": "Structured Execution Logging",
      "description": "As a developer, I want claude-loop to log detailed execution data for every story attempt",
      "acceptanceCriteria": [
        "Create lib/execution-logger.sh for structured logging",
        "Log: story_id, timestamp, success/failure, duration_ms",
        "Log: attempted_actions with tool names and parameters",
        "Log: error_type classification (timeout, not_found, permission, parse, unknown)",
        "Log: error_message and stack trace when available",
        "Log: context (application, file_types, tools_used)",
        "Store logs in .claude-loop/execution_log.jsonl (append-only)",
        "Add log_execution_start() and log_execution_end() hooks to main loop",
        "Include retry count and fallback attempts in log"
      ],
      "priority": 1,
      "dependencies": [],
      "fileScope": ["lib/execution-logger.sh", "claude-loop.sh"],
      "estimatedComplexity": "medium",
      "suggestedModel": "sonnet",
      "passes": true,
      "notes": "Implemented in commit f51f633 - Created lib/execution-logger.sh with JSONL logging, error classification, action tracking, and CLI interface"
    },
    {
      "id": "SI-002",
      "title": "Failure Classification Taxonomy",
      "description": "As a developer, I want failures automatically classified into actionable categories",
      "acceptanceCriteria": [
        "Create lib/failure-classifier.py module",
        "Define FailureCategory enum: SUCCESS, TASK_FAILURE, CAPABILITY_GAP, TRANSIENT_ERROR, UNKNOWN",
        "Implement classify_failure(execution_log_entry) -> FailureCategory",
        "Use heuristics: repeated same error = CAPABILITY_GAP, one-off = TRANSIENT",
        "Use error patterns: 'not found' + UI context = likely CAPABILITY_GAP",
        "Use task analysis: impossible requirements = TASK_FAILURE",
        "Return confidence score (0-1) with classification",
        "Add CLI: python lib/failure-classifier.py classify <log_entry_id>",
        "Add CLI: python lib/failure-classifier.py batch-classify --since <date>"
      ],
      "priority": 2,
      "dependencies": ["SI-001"],
      "fileScope": ["lib/failure-classifier.py"],
      "estimatedComplexity": "complex",
      "suggestedModel": "opus",
      "passes": true,
      "notes": "Implemented in commit 05a7014 - Created lib/failure-classifier.py with FailureCategory enum, classify_failure() with heuristics (pattern matching, frequency analysis, context analysis), CLI for classify and batch-classify"
    },
    {
      "id": "SI-003",
      "title": "Failure Pattern Clustering",
      "description": "As a developer, I want similar failures grouped into patterns for analysis",
      "acceptanceCriteria": [
        "Create lib/pattern-clusterer.py module",
        "Implement cluster_failures(logs) -> List[FailurePattern]",
        "Cluster by: error_type + context similarity",
        "Use text similarity on error messages (fuzzy matching)",
        "FailurePattern dataclass: pattern_id, description, occurrences, first_seen, last_seen",
        "Require minimum 3 occurrences before creating pattern (configurable)",
        "Merge similar patterns automatically (>80% message similarity)",
        "Add CLI: python lib/pattern-clusterer.py analyze --min-occurrences 3",
        "Output patterns as JSON with example failures for each"
      ],
      "priority": 3,
      "dependencies": ["SI-001", "SI-002"],
      "fileScope": ["lib/pattern-clusterer.py"],
      "estimatedComplexity": "medium",
      "suggestedModel": "sonnet",
      "passes": true,
      "notes": "Implemented in commit 6038c1d - Created lib/pattern-clusterer.py with FailurePattern dataclass, similarity clustering, pattern merging, and CLI"
    },
    {
      "id": "SI-004",
      "title": "Root Cause Analysis Engine",
      "description": "As a developer, I want automated 5-Whys analysis on failure patterns",
      "acceptanceCriteria": [
        "Create lib/root-cause-analyzer.py module",
        "Implement analyze_root_cause(pattern) -> RootCauseAnalysis",
        "Use LLM to perform 5-Whys decomposition on failure pattern",
        "RootCauseAnalysis: whys (list of 5), root_cause, capability_gap, confidence",
        "Include counterfactual: 'What capability would have prevented this?'",
        "Reference similar past patterns that were successfully resolved",
        "Add CLI: python lib/root-cause-analyzer.py analyze <pattern_id>",
        "Cache analysis results to avoid repeated LLM calls",
        "Support --no-llm flag for offline analysis using heuristics only"
      ],
      "priority": 4,
      "dependencies": ["SI-003"],
      "fileScope": ["lib/root-cause-analyzer.py"],
      "estimatedComplexity": "complex",
      "suggestedModel": "opus",
      "passes": true,
      "notes": "Implemented in commit f0320ef - Created lib/root-cause-analyzer.py with 5-Whys decomposition, LLM and heuristic analysis, caching, counterfactual analysis, and CLI"
    },
    {
      "id": "SI-005",
      "title": "Capability Gap Generalizer",
      "description": "As a developer, I want specific failures generalized to broader capability categories",
      "acceptanceCriteria": [
        "Create lib/gap-generalizer.py module",
        "Implement generalize_gap(root_cause_analysis) -> GeneralizedGap",
        "GeneralizedGap: category, description, affected_task_types, priority_score",
        "Map to capability taxonomy: UI_INTERACTION, FILE_HANDLING, NETWORK, PARSING, etc.",
        "Identify task families that would benefit from fixing this gap",
        "Estimate future failure probability without improvement",
        "Calculate priority: frequency * impact * feasibility",
        "Add CLI: python lib/gap-generalizer.py generalize <root_cause_id>",
        "Maintain capability_gaps.json as registry of known gaps"
      ],
      "priority": 5,
      "dependencies": ["SI-004"],
      "fileScope": ["lib/gap-generalizer.py", ".claude-loop/capability_gaps.json"],
      "estimatedComplexity": "medium",
      "suggestedModel": "sonnet",
      "passes": true,
      "notes": "Implemented in commit 976382f - Created lib/gap-generalizer.py with GeneralizedGap dataclass, capability taxonomy (10 categories), priority calculation (frequency*impact*feasibility), task family mapping, gap registry, and comprehensive CLI"
    },
    {
      "id": "SI-006",
      "title": "Improvement PRD Generator",
      "description": "As a developer, I want PRDs automatically generated from capability gaps",
      "acceptanceCriteria": [
        "Create lib/improvement-prd-generator.py module",
        "Implement generate_prd(generalized_gap) -> PRD",
        "Generate 5-15 user stories with acceptance criteria",
        "Stories must be generalizable (not specific to one use case)",
        "Include test cases for validation in each story",
        "Set story dependencies based on logical order",
        "Assign complexity and suggested model per story",
        "Calculate PRD priority from gap priority score",
        "Save PRDs to .claude-loop/improvements/ with status=pending_review",
        "Add CLI: python lib/improvement-prd-generator.py generate <gap_id>"
      ],
      "priority": 6,
      "dependencies": ["SI-005"],
      "fileScope": ["lib/improvement-prd-generator.py"],
      "estimatedComplexity": "complex",
      "suggestedModel": "opus",
      "passes": true,
      "notes": "Implemented in commit 5e73442 - Created lib/improvement-prd-generator.py with generate_prd(), story templates for all 10 categories, dependency management, complexity/model assignment, and full CLI (generate, list, show, pending, summary, approve, reject, start, complete)"
    },
    {
      "id": "SI-007",
      "title": "Improvement Review Interface",
      "description": "As a developer, I want a CLI to review and approve improvement PRDs",
      "acceptanceCriteria": [
        "Add --list-improvements flag to claude-loop.sh",
        "Show: PRD name, status, priority, story count, created date",
        "Add --review-improvement <prd_name> to view PRD details",
        "Add --approve-improvement <prd_name> to mark as approved",
        "Add --reject-improvement <prd_name> --reason <text> to reject with feedback",
        "Add --execute-improvement <prd_name> to run approved PRD",
        "Track improvement status: pending_review, approved, rejected, in_progress, complete",
        "Show improvement history with outcomes",
        "Email/notification option when new improvements are generated (optional)"
      ],
      "priority": 7,
      "dependencies": ["SI-006"],
      "fileScope": ["claude-loop.sh", "lib/improvement-manager.sh"],
      "estimatedComplexity": "medium",
      "suggestedModel": "sonnet",
      "passes": true,
      "notes": "Implemented - Created lib/improvement-manager.sh, added 7 CLI flags to claude-loop.sh, history tracking in improvement_history.jsonl"
    },
    {
      "id": "SI-008",
      "title": "Background Gap Analysis Daemon",
      "description": "As a developer, I want gap analysis to run in background without blocking active work",
      "acceptanceCriteria": [
        "Create lib/gap-analysis-daemon.sh for background analysis",
        "Run analysis pipeline on new execution logs periodically (configurable interval)",
        "Default: analyze every 1 hour or after 10 new log entries",
        "Generate PRDs for any new capability gaps discovered",
        "Write status to .claude-loop/daemon_status.json",
        "Support --start-daemon, --stop-daemon, --daemon-status flags",
        "Use lockfile to prevent multiple daemon instances",
        "Log daemon activity to .claude-loop/daemon.log",
        "Graceful shutdown on SIGTERM/SIGINT"
      ],
      "priority": 8,
      "dependencies": ["SI-006"],
      "fileScope": ["lib/gap-analysis-daemon.sh"],
      "estimatedComplexity": "medium",
      "suggestedModel": "sonnet",
      "passes": true,
      "notes": "Implemented in commit f7d75f5 - Created lib/gap-analysis-daemon.sh with periodic analysis, configurable interval (DAEMON_INTERVAL_SECONDS), log entry threshold (DAEMON_LOG_THRESHOLD), lockfile, graceful shutdown, and CLI integration in claude-loop.sh"
    },
    {
      "id": "SI-009",
      "title": "Improvement Validation Suite",
      "description": "As a developer, I want improvements validated before deployment",
      "acceptanceCriteria": [
        "Create lib/improvement-validator.py module",
        "Implement validate_improvement(prd_path) -> ValidationResult",
        "Run existing test suite - must not regress",
        "Run improvement-specific tests from PRD",
        "Test against held-out failure cases (should now succeed)",
        "Measure capability coverage before/after",
        "Generate validation report with pass/fail details",
        "Add --validate flag to --execute-improvement",
        "Block deployment if validation fails",
        "Support --force flag to override (with warning)"
      ],
      "priority": 9,
      "dependencies": ["SI-007"],
      "fileScope": ["lib/improvement-validator.py"],
      "estimatedComplexity": "medium",
      "suggestedModel": "sonnet",
      "passes": true,
      "notes": "Implemented in commit - Created lib/improvement-validator.py with ValidationResult dataclass, test framework detection, pytest parsing, held-out case validation, coverage measurement, validation reports, CLI with validate/check-tests/check-coverage/reports/add-held-out commands, and integration with improvement-manager.sh and claude-loop.sh (--validate-improvement, --validate, --force flags)"
    },
    {
      "id": "SI-010",
      "title": "Capability Inventory Tracker",
      "description": "As a developer, I want an inventory of claude-loop's current capabilities",
      "acceptanceCriteria": [
        "Create lib/capability-inventory.py module",
        "Define capability taxonomy: categories, subcategories, skills",
        "Auto-discover capabilities from MCP servers, tools, agents",
        "Track capability status: available, limited, unavailable",
        "Map each capability to required dependencies",
        "Update inventory when improvements are deployed",
        "Add CLI: python lib/capability-inventory.py list",
        "Add CLI: python lib/capability-inventory.py check <task_description>",
        "Output: required capabilities vs. available for a task"
      ],
      "priority": 10,
      "dependencies": ["SI-005"],
      "fileScope": ["lib/capability-inventory.py", ".claude-loop/capability_inventory.json"],
      "estimatedComplexity": "complex",
      "suggestedModel": "opus",
      "passes": true,
      "notes": "Implemented lib/capability-inventory.py with: CapabilityCategory enum (13 categories), SUBCATEGORIES mapping with skills, auto-discovery from builtin tools (Read, Write, Edit, Bash, Glob, Grep, Task, WebSearch, mcp__ide__getDiagnostics, AskUserQuestion), agents, MCP servers, skills directory, and lib/ scripts. Tracks status (available/limited/unavailable/unknown) with dependencies. CLI: list (with filters), check <task>, discover, show, status, categories, update-status. Stores inventory in .claude-loop/capability_inventory.json. Enables predictive gap detection by matching task requirements against available capabilities."
    },
    {
      "id": "SI-011",
      "title": "Self-Improvement Dashboard",
      "description": "As a developer, I want a visual dashboard showing improvement status and metrics",
      "acceptanceCriteria": [
        "Add /improvements endpoint to dashboard/app.py",
        "Show: pending PRDs, approved, in-progress, completed",
        "Show: failure patterns discovered, gaps identified",
        "Show: improvement success rate over time",
        "Show: capability coverage trend",
        "Show: top 5 unresolved capability gaps by priority",
        "Interactive: click to view PRD details",
        "Interactive: approve/reject from dashboard",
        "Export metrics as JSON for external monitoring"
      ],
      "priority": 11,
      "dependencies": ["SI-007", "SI-010"],
      "fileScope": ["dashboard/app.py", "dashboard/templates/improvements.html"],
      "estimatedComplexity": "medium",
      "suggestedModel": "sonnet",
      "passes": true,
      "notes": "Implemented /improvements endpoint with: PRD status tabs (pending, approved, in-progress, completed), failure patterns by error type, top 5 capability gaps by priority, success rate over time chart, capability coverage by category, recent activity log, and JSON export endpoints (/api/improvements, /api/improvements/prds, /api/improvements/gaps, /api/improvements/patterns, /api/improvements/coverage, /api/improvements/metrics)"
    },
    {
      "id": "SI-012",
      "title": "Rollback Mechanism",
      "description": "As a developer, I want to rollback improvements that cause regressions",
      "acceptanceCriteria": [
        "Create lib/improvement-rollback.py module",
        "Track git commits associated with each improvement",
        "Implement rollback_improvement(prd_name) -> RollbackResult",
        "Revert all commits from the improvement branch",
        "Re-run validation suite after rollback",
        "Update capability inventory to reflect rollback",
        "Mark improvement as 'rolled_back' with reason",
        "Add --rollback-improvement <prd_name> to CLI",
        "Notify when rollback occurs (optional)",
        "Keep rollback history for analysis"
      ],
      "priority": 12,
      "dependencies": ["SI-009"],
      "fileScope": ["lib/improvement-rollback.py"],
      "estimatedComplexity": "medium",
      "suggestedModel": "sonnet",
      "passes": true,
      "notes": "Implemented lib/improvement-rollback.py with: ImprovementRollback class, CommitInfo/RollbackResult dataclasses, rollback_improvement() function that reverts commits in reverse order, post-rollback validation via improvement-validator.py, capability inventory updates to mark capabilities as limited, PRD status update to rolled_back with reason, rollback history in rollback_history.jsonl. CLI: rollback/status/commits/track/start-tracking/complete-tracking/history/list commands with --dry-run/--reason/--skip-validation flags. Integrated into claude-loop.sh via --rollback-improvement flag and improvement-manager.sh rollback command."
    },
    {
      "id": "SI-013",
      "title": "Classification Accuracy Validator",
      "description": "As a developer, I want to measure and improve failure classification accuracy",
      "acceptanceCriteria": [
        "Create tests/test_failure_classification.py",
        "Include 20+ labeled test cases (manual ground truth)",
        "Test cases cover: SUCCESS, TASK_FAILURE, CAPABILITY_GAP, TRANSIENT",
        "Measure classification accuracy, precision, recall per category",
        "Require >80% accuracy before enabling autonomous PRD generation",
        "Add --validate-classifier flag to run accuracy test",
        "Generate confusion matrix visualization",
        "Track accuracy over time as classifier improves",
        "Alert if accuracy drops below threshold"
      ],
      "priority": 13,
      "dependencies": ["SI-002"],
      "fileScope": ["tests/test_failure_classification.py", "tests/fixtures/labeled_failures.json"],
      "estimatedComplexity": "medium",
      "suggestedModel": "sonnet",
      "passes": true,
      "notes": "Implemented with 24 labeled test cases covering all 4 categories. Classification accuracy: 95.83% (23/24 correct), well above 80% threshold. Per-category: CAPABILITY_GAP 90.91%, SUCCESS 100%, TASK_FAILURE 100%, TRANSIENT_ERROR 100%. Generates confusion matrix visualization, tracks accuracy over time in .claude-loop/accuracy_history.jsonl, alerts if accuracy drops below 70%."
    },
    {
      "id": "SI-014",
      "title": "Autonomous Mode Gate",
      "description": "As a developer, I want safeguards before enabling fully autonomous self-improvement",
      "acceptanceCriteria": [
        "Create lib/autonomous-gate.py to check readiness",
        "Gate criteria: classification accuracy >80%",
        "Gate criteria: at least 3 successful improvement cycles completed",
        "Gate criteria: zero rollbacks in last 5 improvements",
        "Gate criteria: explicit user opt-in via config",
        "If gates pass, allow --autonomous flag for daemon",
        "Autonomous mode: auto-approve improvements below risk threshold",
        "Risk threshold configurable (default: only low-risk improvements)",
        "Log all autonomous decisions for audit",
        "Kill switch: --disable-autonomous to revert to human-approval"
      ],
      "priority": 14,
      "dependencies": ["SI-008", "SI-012", "SI-013"],
      "fileScope": ["lib/autonomous-gate.py", "config.json"],
      "estimatedComplexity": "medium",
      "suggestedModel": "sonnet",
      "passes": true,
      "notes": "Implemented lib/autonomous-gate.py with: GateCriteria enum (CLASSIFICATION_ACCURACY, SUCCESSFUL_CYCLES, ZERO_ROLLBACKS, USER_OPT_IN), GateCheckResult and GateResult dataclasses, check_all_gates() function, auto_approve() function with risk threshold (default: priority < 5). CLI commands: status, check, approve <prd>, enable, disable, history, set-threshold. Integrated into claude-loop.sh with --autonomous, --disable-autonomous, --autonomous-status flags. Daemon integration in gap-analysis-daemon.sh for auto-approval when DAEMON_AUTONOMOUS_MODE=true. Audit logging to .claude-loop/autonomous_decisions.jsonl."
    },
    {
      "id": "SI-015",
      "title": "Integration Tests for Self-Improvement Pipeline",
      "description": "As a developer, I want end-to-end tests for the self-improvement pipeline",
      "acceptanceCriteria": [
        "Create tests/test_self_improvement_e2e.py",
        "Test: log failure -> classify -> cluster -> analyze -> generate PRD",
        "Test: PRD review workflow (approve, reject, execute)",
        "Test: validation suite blocks bad improvements",
        "Test: rollback restores previous state",
        "Test: daemon runs in background without interference",
        "Test: dashboard shows correct metrics",
        "Use mock LLM responses for deterministic tests",
        "CI mode: run without external dependencies",
        "Document test requirements in README"
      ],
      "priority": 15,
      "dependencies": ["SI-001", "SI-002", "SI-003", "SI-004", "SI-005", "SI-006", "SI-007"],
      "fileScope": ["tests/test_self_improvement_e2e.py"],
      "estimatedComplexity": "complex",
      "suggestedModel": "opus",
      "passes": true,
      "notes": "Implemented comprehensive E2E tests with 21 test cases covering full pipeline, PRD workflows, validation, rollback, daemon, mock LLM integration, and realistic scenarios. All tests pass. Documentation in tests/TEST_REQUIREMENTS.md."
    }
  ]
}
