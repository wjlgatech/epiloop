{
  "project": "stratified-memory-v2",
  "branchName": "feature/stratified-memory-v2",
  "description": "Revised scalable self-improvement architecture addressing: domain-contextualized aggregation, experience feedback loops, privacy-first design, human-gated promotions, conflict detection, and leading indicators. Key principle: Experienceâ†’Code promotion is ALWAYS human-gated until demonstrated 95% alignment.",
  "parallelization": {
    "enabled": true,
    "maxWorkers": 2
  },
  "userStories": [
    {
      "id": "SCALE-001",
      "title": "Domain-Contextualized Experience Store",
      "description": "As a developer, I want experiences tagged with domain context so similar symptoms from different domains aren't conflated",
      "acceptanceCriteria": [
        "Create lib/experience-store.py with domain-aware storage",
        "ExperienceEntry dataclass: problem_signature, solution_approach, domain_context, tool_context, success_count, retrieval_count, helpful_count, last_used",
        "domain_context includes: project_type (unity, web, ml, robotics, etc.), language, frameworks, tools_used",
        "Use lightweight vector DB (ChromaDB file-based, no server required)",
        "Embedding via sentence-transformers with domain prefix: '[unity] UI element not found'",
        "Store in .claude-loop/experiences/ with domain-partitioned collections",
        "CLI: experience store <problem> <solution> --domain <context_json>",
        "CLI: experience stats --by-domain (show distribution)",
        "Retrieval filters by domain first, then similarity",
        "Limit total DB to 500MB with per-domain LRU eviction"
      ],
      "priority": 1,
      "dependencies": [],
      "fileScope": ["lib/experience-store.py", ".claude-loop/experiences/"],
      "estimatedComplexity": "complex",
      "suggestedModel": "opus",
      "passes": true,
      "notes": "Implemented in commit 9186d53. Domain-contextualized storage with domain_context, retrieval_count, helpful_count. Supports domain filtering, domain-prefixed embeddings, per-domain LRU eviction, CLI --domain flag, stats --by-domain."
    },
    {
      "id": "SCALE-002",
      "title": "Experience Retrieval with Feedback Loop",
      "description": "As a developer, I want to track whether retrieved experiences actually helped so the system learns retrieval quality",
      "acceptanceCriteria": [
        "Create lib/experience-retriever.py with outcome tracking",
        "retrieve_similar(problem, domain_context, k=5) -> List[ExperienceEntry]",
        "Filter by domain match first (exact or parent category)",
        "Similarity threshold: only return if cosine > 0.75 (higher than original)",
        "Rank by: similarity * (helpful_count / max(retrieval_count, 1)) * recency_factor",
        "mark_retrieval_outcome(experience_id, outcome: 'used'|'ignored'|'helped'|'hurt')",
        "Decay experiences with low helpful_rate (<20% after 10+ retrievals)",
        "CLI: experience search <problem> --domain <context>",
        "CLI: experience feedback <experience_id> --outcome helped",
        "CLI: experience quality-report (show retrieval hit rate, helpful rate by domain)",
        "Log retrieval outcomes in .claude-loop/retrieval_outcomes.jsonl"
      ],
      "priority": 2,
      "dependencies": ["SCALE-001"],
      "fileScope": ["lib/experience-retriever.py"],
      "estimatedComplexity": "medium",
      "suggestedModel": "sonnet",
      "passes": true,
      "notes": "Implemented in commit 4f10010. Created lib/experience-retriever.py with retrieve_similar(), domain filtering, ranking by similarity*helpful_rate*recency_factor, mark_retrieval_outcome(), decay_low_quality_experiences(), CLI commands (search, feedback, quality-report, decay, history), and JSONL outcome logging."
    },
    {
      "id": "SCALE-003",
      "title": "Automatic Experience Recording with Domain Detection",
      "description": "As a developer, I want successful solutions auto-recorded with detected domain context",
      "acceptanceCriteria": [
        "Create lib/experience-recorder.py with domain auto-detection",
        "detect_domain(project_path) -> DomainContext by analyzing: package.json, requirements.txt, *.csproj, file patterns",
        "Domain taxonomy: web_frontend, web_backend, unity_game, unity_xr, isaac_sim, ml_training, ml_inference, data_pipeline, cli_tool, other",
        "Hook into execution completion: on story passes=true",
        "Extract problem signature from story + error context",
        "Extract solution approach from git diff summary (not full diff)",
        "Skip if: <10 lines changed, or purely config change, or test-only",
        "Deduplicate: if >0.9 similarity in same domain exists, increment counts instead",
        "CLI: experience record-from-log <log_entry_id>",
        "CLI: experience detect-domain (show detected domain for current project)",
        "Record domain confidence score (high/medium/low) based on detection signals"
      ],
      "priority": 3,
      "dependencies": ["SCALE-001"],
      "fileScope": ["lib/experience-recorder.py", "lib/domain-detector.py"],
      "estimatedComplexity": "medium",
      "suggestedModel": "sonnet",
      "passes": true,
      "notes": "Implemented in commit - Created lib/domain-detector.py with DomainDetector class supporting web_frontend, web_backend, unity_game, unity_xr, isaac_sim, ml_training, ml_inference, data_pipeline, cli_tool, robotics, other domains. Created lib/experience-recorder.py with ExperienceRecorder class: detect_domain(), should_record_story() with skip rules (<10 lines, config-only, test-only, >0.9 similarity dedup), record_from_story(), record_from_log_entry(). CLI: detect-domain, record-story, record-from-log, record, check commands. Domain confidence levels: high/medium/low based on detection signals."
    },
    {
      "id": "SCALE-004",
      "title": "Privacy-First Local-Only Architecture",
      "description": "As an enterprise user, I want all data to stay local by default with no external communication",
      "acceptanceCriteria": [
        "Create lib/privacy-config.py for privacy settings",
        "Default mode: FULLY_LOCAL (no network calls for improvement system)",
        "All experience data stored locally in .claude-loop/",
        "No telemetry, no analytics, no phone-home by default",
        "Optional TEAM_SYNC mode: sync via shared folder or git (no cloud)",
        "Optional FEDERATED mode: requires explicit enterprise agreement",
        "Privacy audit: CLI command to show all data that would be shared",
        "CLI: privacy status (show current mode and data locations)",
        "CLI: privacy audit --export (generate report of all stored data)",
        "Data export: experience export --format portable (for migration)",
        "Data delete: experience purge --confirm (complete removal)",
        "Document privacy guarantees in PRIVACY.md"
      ],
      "priority": 4,
      "dependencies": [],
      "fileScope": ["lib/privacy-config.py", "PRIVACY.md"],
      "estimatedComplexity": "medium",
      "suggestedModel": "sonnet",
      "passes": true,
      "notes": "Implemented in commit 4653170. Created lib/privacy-config.py with PrivacyMode enum (FULLY_LOCAL, TEAM_SYNC, FEDERATED), PrivacyManager class with status, audit, export, purge functions. CLI: status, audit --export, export --output, purge --confirm, set-mode. PRIVACY.md documents all privacy guarantees."
    },
    {
      "id": "SCALE-005",
      "title": "Domain Adapter Extension System",
      "description": "As a developer, I want domain-specific capabilities as separate versioned packages",
      "acceptanceCriteria": [
        "Create lib/domain-adapter.py for adapter loading",
        "Adapter manifest: adapter.json with name, version, domain, capabilities, maintainer",
        "Adapters directory: adapters/{domain}/ with isolated code",
        "Adapter can provide: prompts, tools, validators, experience embeddings",
        "Adapters are versioned independently (semver)",
        "Adapters cannot modify core claude-loop files",
        "CLI: adapter list (show installed adapters)",
        "CLI: adapter enable <name> / adapter disable <name>",
        "CLI: adapter update <name> (update to latest)",
        "CLI: adapter info <name> (show manifest, changelog)",
        "Load adapters based on detected project domain",
        "Graceful degradation if adapter unavailable"
      ],
      "priority": 5,
      "dependencies": [],
      "fileScope": ["lib/domain-adapter.py", "adapters/"],
      "estimatedComplexity": "complex",
      "suggestedModel": "opus",
      "passes": true,
      "notes": "Implemented in commit - Created lib/domain-adapter.py with AdapterManager class: adapter manifest (adapter.json), semver versioning, capability types (prompts, tools, validators, embeddings), core file protection, graceful degradation. CLI commands: list, enable, disable, update, info, load-for-domain, prompts, validators, tools. Created adapters/example/ with full example adapter. 36 unit tests in tests/test_domain_adapter.py."
    },
    {
      "id": "SCALE-006",
      "title": "Physical AI Domain Adapter",
      "description": "As a Physical AI developer, I want specialized support for Isaac Sim, robotics, and XR workflows",
      "acceptanceCriteria": [
        "Create adapters/physical-ai/ directory",
        "Support domains: isaac_sim, ros2, unity_xr, nvidia_omniverse",
        "Domain-specific prompts for USD handling, URDF parsing, sensor simulation",
        "Domain-specific experience embeddings (fine-tuned or prefix-enhanced)",
        "Validators for common Physical AI errors (USD prim paths, ROS topic types)",
        "Tool integrations: Isaac Sim CLI, ROS2 commands, Unity Editor automation",
        "Experience templates for common Physical AI problems",
        "CLI: adapter install physical-ai",
        "Document in adapters/physical-ai/README.md",
        "Include example experiences for bootstrapping"
      ],
      "priority": 6,
      "dependencies": ["SCALE-005"],
      "fileScope": ["adapters/physical-ai/"],
      "estimatedComplexity": "complex",
      "suggestedModel": "opus",
      "passes": true,
      "notes": "Implemented in commit d418b1f - Created adapters/physical-ai/ with prompts (USD, URDF, sensors, ROS2, Unity XR), validators (usd_prim, ros2_topic, urdf_structure), tools (isaac-sim-cli, ros2-commands, unity-editor-automation, urdf-tools), embeddings config, and 15 bootstrap experiences"
    },
    {
      "id": "SCALE-007",
      "title": "Human-Gated Improvement Queue",
      "description": "As a maintainer, I want all improvement proposals queued for human review, not auto-executed",
      "acceptanceCriteria": [
        "Create lib/improvement-queue.py for queued improvements",
        "ImprovementProposal dataclass: id, problem_pattern, proposed_solution, affected_domains, confidence, evidence_count, created_at, status",
        "Status lifecycle: proposed -> reviewed -> approved/rejected -> implemented/archived",
        "NO automatic implementation - all proposals require human approval",
        "Queue persisted in .claude-loop/improvement_queue.json",
        "Weekly digest: CLI: improvement digest (summary of pending proposals)",
        "CLI: improvement queue list (show all pending)",
        "CLI: improvement queue review <id> (show details for review)",
        "CLI: improvement queue approve <id> --notes <text>",
        "CLI: improvement queue reject <id> --reason <text>",
        "Track reviewer decisions for calibration measurement",
        "Dashboard: /improvement-queue showing pending proposals"
      ],
      "priority": 7,
      "dependencies": [],
      "fileScope": ["lib/improvement-queue.py"],
      "estimatedComplexity": "medium",
      "suggestedModel": "sonnet",
      "passes": true,
      "notes": "Implemented in commit - Created lib/improvement-queue.py with ImprovementProposal dataclass (id, problem_pattern, proposed_solution, affected_domains, confidence, evidence_count, status, priority_score, etc.), ReviewerDecision for calibration tracking, ImprovementQueueManager with propose/review/approve/reject/mark_implemented/archive lifecycle. CLI: queue list/review/approve/reject, propose, digest, stats, calibration. Queue persisted in .claude-loop/improvement_queue.json, decisions logged to .claude-loop/improvement_decisions.jsonl for 95% alignment threshold tracking. 28 unit tests in tests/test_improvement_queue.py."
    },
    {
      "id": "SCALE-008",
      "title": "Promotion Criteria with Lifecycle Cost",
      "description": "As a maintainer, I want promotion decisions to include maintenance cost, not just usage",
      "acceptanceCriteria": [
        "Create lib/promotion-evaluator.py with comprehensive criteria",
        "PromotionCriteria dataclass: usage_count, success_rate, domain_spread, maintenance_cost_estimate, dependency_risk, reversibility_score, conflict_potential",
        "maintenance_cost_estimate: based on code complexity, external dependencies, update frequency",
        "dependency_risk: does this depend on unstable APIs or tools?",
        "reversibility_score: can we cleanly remove this if wrong? (0-1)",
        "conflict_potential: does this contradict existing improvements?",
        "Weighted score combining all factors (not just usage threshold)",
        "CLI: improvement evaluate <id> (show full criteria breakdown)",
        "CLI: improvement compare <id1> <id2> (side-by-side comparison)",
        "Require all criteria above threshold for promotion recommendation",
        "Human makes final decision, system provides analysis"
      ],
      "priority": 8,
      "dependencies": ["SCALE-007"],
      "fileScope": ["lib/promotion-evaluator.py"],
      "estimatedComplexity": "complex",
      "suggestedModel": "opus",
      "passes": true,
      "notes": "Implemented in commit 2c1c50d - Created lib/promotion-evaluator.py with PromotionCriteria dataclass (usage_count, success_rate, domain_spread, maintenance_cost, dependency_risk, reversibility, conflict_potential), MaintenanceCostEstimate with complexity analysis, DependencyRiskAssessment with API stability checks, ReversibilityAssessment with state/API/DB change detection, ConflictAssessment with behavioral and scope overlap detection. CLI: evaluate <id>, compare <id1> <id2>, thresholds, list. Weighted scoring (100 points total) with configurable thresholds. Human makes final decision."
    },
    {
      "id": "SCALE-009",
      "title": "Conflict Detection System",
      "description": "As a maintainer, I want conflicts between improvements detected before promotion",
      "acceptanceCriteria": [
        "Create lib/conflict-detector.py for improvement conflict analysis",
        "ImprovementScope dataclass: preconditions, affected_behaviors, domain_applicability, exclusions",
        "Require scope declaration for all improvement proposals",
        "detect_conflicts(improvement_a, improvement_b) -> List[Conflict]",
        "Conflict types: behavioral_contradiction, scope_overlap, dependency_conflict, resource_contention",
        "Example: 'always retry' vs 'never retry idempotent' detected as behavioral_contradiction",
        "CLI: improvement conflicts <id> (show conflicts with existing)",
        "CLI: improvement scope <id> (show/edit scope declaration)",
        "Block promotion if unresolved conflicts exist",
        "Suggest resolution strategies: scope narrowing, conditional application, merge",
        "Conflict resolution requires human decision"
      ],
      "priority": 9,
      "dependencies": ["SCALE-007"],
      "fileScope": ["lib/conflict-detector.py"],
      "estimatedComplexity": "complex",
      "suggestedModel": "opus",
      "passes": true,
      "notes": "Implemented in commit b1030e8 - Created lib/conflict-detector.py with ImprovementScope dataclass (preconditions, affected_behaviors, domain_applicability, exclusions, resources_used, effects), Conflict dataclass with 4 types (behavioral_contradiction, scope_overlap, dependency_conflict, resource_contention), ConflictDetector with detect_conflicts(), analyze_improvement(), can_promote(). Resolution strategies: scope_narrowing, conditional, merge, prioritize, domain_split, manual_review. CLI: conflicts, scope (show/add/infer/list), can-promote, list, resolve, detect. Blocks promotion if unresolved blocking conflicts. 40 unit tests."
    },
    {
      "id": "SCALE-010",
      "title": "Human-Assisted Pattern Clustering",
      "description": "As a maintainer, I want pattern clustering surfaced for review, not auto-executed",
      "acceptanceCriteria": [
        "Create lib/pattern-clustering.py with human-in-the-loop",
        "Cluster improvements by problem similarity (embedding-based)",
        "ClusterProposal: cluster_id, member_improvements, proposed_generalization, confidence, requires_human_validation",
        "If confidence < 0.8: flag for mandatory human review",
        "CLI: improvement clusters (show proposed clusters)",
        "CLI: improvement cluster review <cluster_id> (detailed review)",
        "CLI: improvement cluster approve <cluster_id> --generalization <text>",
        "CLI: improvement cluster reject <cluster_id> --reason <text>",
        "NO automatic compression - humans decide what generalizes",
        "Track clustering accuracy: did human agree with proposed cluster?",
        "Surface uncertain clusters weekly for human validation"
      ],
      "priority": 10,
      "dependencies": ["SCALE-007"],
      "fileScope": ["lib/pattern-clustering.py"],
      "estimatedComplexity": "complex",
      "suggestedModel": "opus",
      "passes": true,
      "notes": "Implemented in commit - Created lib/pattern-clustering.py with PatternClusteringManager class: ClusterProposal dataclass (cluster_id, member_improvements, proposed_generalization, confidence, requires_human_validation, status), ClusterMember dataclass, embedding-based similarity clustering with text fallback, confidence < 0.8 flags for mandatory human validation. CLI: clusters, cluster review/approve/reject, analyze, accuracy, stats, weekly. NO automatic compression - humans decide generalizations. ClusterDecision tracks accuracy. 38 unit tests in tests/test_pattern_clustering.py."
    },
    {
      "id": "SCALE-011",
      "title": "Leading Indicator Metrics",
      "description": "As a maintainer, I want early warning indicators, not just lagging metrics",
      "acceptanceCriteria": [
        "Create lib/health-indicators.py for system health monitoring",
        "Leading indicators: proposal_rate_change, cluster_concentration, retrieval_miss_rate, domain_drift",
        "proposal_rate_change: >2x spike in improvement proposals = something wrong",
        "cluster_concentration: are failures spreading or concentrating in one area?",
        "retrieval_miss_rate: how often does experience store return nothing useful?",
        "domain_drift: is the system seeing new domains it wasn't trained for?",
        "Alert thresholds configurable per indicator",
        "CLI: health status (show all indicators with RAG status)",
        "CLI: health history --days 30 (trend over time)",
        "CLI: health alerts (show active alerts)",
        "Dashboard integration: /health-indicators endpoint",
        "Alert when leading indicators predict problems before they manifest"
      ],
      "priority": 11,
      "dependencies": ["SCALE-001", "SCALE-002", "SCALE-007"],
      "fileScope": ["lib/health-indicators.py", "dashboard/app.py"],
      "estimatedComplexity": "medium",
      "suggestedModel": "sonnet",
      "passes": true,
      "notes": "Implemented in commit 97703cb - Created lib/health-indicators.py with 4 leading indicators (proposal_rate_change, cluster_concentration, retrieval_miss_rate, domain_drift), RAG status with configurable thresholds, alert system, history tracking, CLI commands (status, history, alerts, acknowledge, thresholds, check), dashboard endpoints. 43 unit tests."
    },
    {
      "id": "SCALE-012",
      "title": "Calibration Tracking System",
      "description": "As a maintainer, I want to measure system alignment with human decisions over time",
      "acceptanceCriteria": [
        "Create lib/calibration-tracker.py for alignment measurement",
        "Track: system_recommendation vs human_decision for each review",
        "CalibrationMetric: agreement_rate, false_positive_rate, false_negative_rate, by_domain",
        "Target: 95% agreement before any autonomous promotion allowed",
        "Minimum evaluation period: 6 months of parallel evaluation",
        "Parallel mode: system makes recommendations, humans decide, we compare",
        "CLI: calibration status (show current alignment metrics)",
        "CLI: calibration history (show trend over time)",
        "CLI: calibration disagreements (show cases where system != human)",
        "Block autonomous mode until calibration threshold met",
        "Dashboard: /calibration showing alignment trend",
        "Weekly calibration report emailed/logged"
      ],
      "priority": 12,
      "dependencies": ["SCALE-007", "SCALE-008", "SCALE-010"],
      "fileScope": ["lib/calibration-tracker.py"],
      "estimatedComplexity": "medium",
      "suggestedModel": "sonnet",
      "passes": true,
      "notes": "Implemented in commit - Created lib/calibration-tracker.py with CalibrationTracker class: NormalizedDecision (consolidates decisions from improvement_decisions.jsonl, cluster_decisions.jsonl, promotion_decisions.jsonl), CalibrationMetrics (agreement_rate, false_positive_rate, false_negative_rate, by_source, by_domain, by_confidence, recent_trend), CalibrationSnapshot for history tracking, WeeklyReport for weekly calibration reports. CLI: status, history, disagreements (--type false_positive/false_negative), autonomous-check, weekly-report (--list/--show), snapshot. Dashboard endpoints: /api/calibration, /api/calibration/history, /api/calibration/disagreements, /api/calibration/autonomous-check, /api/calibration/weekly-report. Blocks autonomous mode until 95% agreement over 50+ decisions and 180+ days evaluation. 37 unit tests in tests/test_calibration_tracker.py."
    },
    {
      "id": "SCALE-013",
      "title": "Core Protection with Immutability Rules",
      "description": "As a maintainer, I want core files protected from any automated modification",
      "acceptanceCriteria": [
        "Create lib/core-protection.py with strict immutability",
        "CORE_FILES: claude-loop.sh, lib/execution-logger.sh, lib/core-protection.py, and other foundational files",
        "is_core_file(path) -> bool with clear classification",
        "Block ALL automated modifications to core files",
        "No --force flag to override - core changes require manual commits",
        "Log all core access attempts to .claude-loop/core_access_audit.log",
        "CLI: core list (show protected files)",
        "CLI: core audit (show access attempts)",
        "CLI: core add <path> / core remove <path> (modify protection list, requires confirmation)",
        "Improvement proposals affecting core -> auto-rejected with explanation",
        "Document core modification process in CONTRIBUTING.md"
      ],
      "priority": 13,
      "dependencies": [],
      "fileScope": ["lib/core-protection.py", "CONTRIBUTING.md"],
      "estimatedComplexity": "medium",
      "suggestedModel": "sonnet",
      "passes": true,
      "notes": "Implemented in commit 3203d46. Created lib/core-protection.py with CoreProtectionManager class: CORE_FILES list (16 protected entries, 3 immutable), is_core_file(), check_file() with CheckResult, validate_changes(), block_modification(). Audit logging to .claude-loop/core_access_audit.log. CLI commands: list, check, audit (--stats), add (--confirm), remove (--confirm), validate-changes, check-proposal. Improvement proposals affecting core auto-rejected via check_proposal_affects_core(). Created CONTRIBUTING.md documenting core modification process."
    },
    {
      "id": "SCALE-014",
      "title": "Experience-Augmented Prompts with Domain Context",
      "description": "As a developer, I want relevant past experiences included in prompts with proper domain filtering",
      "acceptanceCriteria": [
        "Modify build_iteration_prompt() in claude-loop.sh",
        "Detect current project domain before retrieval",
        "Retrieve experiences filtered by domain match",
        "Only include experiences with helpful_rate > 30%",
        "Format as: '## Relevant Past Experiences (from similar {domain} projects)'",
        "Include for each: problem summary, solution approach, success rate",
        "Limit to top 3 most relevant (similarity * helpful_rate)",
        "Log: experience_augmented=true, experiences_retrieved=N in execution log",
        "Track: did this augmented run succeed? (for retrieval feedback)",
        "CLI: --no-experience flag to disable augmentation",
        "If no relevant experiences, don't add empty section"
      ],
      "priority": 14,
      "dependencies": ["SCALE-001", "SCALE-002", "SCALE-003"],
      "fileScope": ["claude-loop.sh", "lib/prompt-augmenter.py"],
      "estimatedComplexity": "medium",
      "suggestedModel": "sonnet",
      "passes": true,
      "notes": "Implemented in commit a7a59de. Created lib/prompt-augmenter.py with PromptAugmenter class: automatic domain detection, domain-filtered experience retrieval, quality gating (helpful_rate > 30%), formatted markdown output for prompt inclusion, execution logging for retrieval tracking, CLI commands (augment, check, detect-domain). Updated claude-loop.sh: --no-experience flag, check_prompt_augmenter(), get_experience_augmentation(), experience section prepended to prompts. Unit tests in tests/test_prompt_augmenter.py."
    },
    {
      "id": "SCALE-015",
      "title": "Team Experience Sharing (Local Only)",
      "description": "As a team, I want to share experiences without any cloud services",
      "acceptanceCriteria": [
        "Create lib/experience-sync.py for local/team sync only",
        "Export format: experiences-{domain}-{date}.jsonl.gz (portable)",
        "Import with deduplication and conflict resolution",
        "Sync modes: manual export/import, shared folder watch, git-based",
        "NO cloud sync - only local filesystem or git",
        "Merge strategy: keep higher helpful_rate version on conflict",
        "CLI: experience export --domain <domain> --output <file>",
        "CLI: experience import <file> --merge",
        "CLI: experience sync-folder <path> --watch (file system watcher)",
        "Filter export: --min-helpful-rate 0.3 --min-retrievals 5",
        "Audit log of all sync operations",
        "Document sync protocol in TEAM-SYNC.md"
      ],
      "priority": 15,
      "dependencies": ["SCALE-001", "SCALE-004"],
      "fileScope": ["lib/experience-sync.py", "TEAM-SYNC.md"],
      "estimatedComplexity": "medium",
      "suggestedModel": "sonnet",
      "passes": true,
      "notes": "Implemented in commit 8c9b8f8. Created lib/experience-sync.py with ExperienceSyncManager class supporting: export with experiences-{domain}-{date}.jsonl.gz format, import with deduplication (90% similarity threshold) and conflict resolution (keep higher helpful_rate), sync modes (manual, shared folder watch with watchdog, git-based), quality filters (min_helpful_rate, min_retrievals, domain), audit log to .claude-loop/sync_audit.jsonl, CLI commands (export, import, sync-folder, history, stats, config). TEAM-SYNC.md documentation included. 28 unit tests in tests/test_experience_sync.py."
    },
    {
      "id": "SCALE-016",
      "title": "Comprehensive Scale Architecture Tests",
      "description": "As a developer, I want thorough tests ensuring the scale architecture works correctly",
      "acceptanceCriteria": [
        "Create tests/test_scale_architecture_v2.py",
        "Test: domain-contextualized experience storage and retrieval",
        "Test: retrieval feedback loop updates helpful_rate correctly",
        "Test: domain auto-detection accuracy across project types",
        "Test: privacy config enforces local-only by default",
        "Test: domain adapter loading and isolation",
        "Test: improvement queue workflow (propose -> review -> approve/reject)",
        "Test: promotion criteria calculation with all factors",
        "Test: conflict detection catches behavioral contradictions",
        "Test: pattern clustering flags low-confidence for human review",
        "Test: leading indicators calculate correctly",
        "Test: calibration tracking measures alignment accurately",
        "Test: core protection blocks all automated modifications",
        "Use mock embeddings and deterministic test data",
        "Coverage target: >85% for all new modules"
      ],
      "priority": 16,
      "dependencies": ["SCALE-001", "SCALE-002", "SCALE-003", "SCALE-004", "SCALE-005", "SCALE-007", "SCALE-008", "SCALE-009", "SCALE-010", "SCALE-011", "SCALE-012", "SCALE-013"],
      "fileScope": ["tests/test_scale_architecture_v2.py"],
      "estimatedComplexity": "complex",
      "suggestedModel": "opus",
      "passes": true,
      "notes": "Implemented in tests/test_scale_architecture_v2.py. Created 57 unit tests covering: TestExperienceStore (domain context CRUD, record/retrieve with domain), TestDomainDetector (web_frontend, web_backend, ml_training detection, confidence scores), TestPrivacyConfig (FULLY_LOCAL default, network blocking), TestDomainAdapter (discover, load_for_domain, disable/enable, prompts), TestImprovementQueue (propose/review/approve/reject workflow, deduplication), TestConflictDetector (scope creation, set/get scope, detect_conflicts), TestPatternClustering (analyze_and_cluster), TestHealthIndicators (RAG status indicators), TestCalibrationTracker (calculate_metrics, autonomous eligibility, disagreements), TestCoreProtection (is_core_file, check_file, pattern matching, secrets protection), TestPromotionEvaluator (evaluate, recommendations), TestScaleArchitectureIntegration (end-to-end flows). All 57 tests pass."
    }
  ]
}
